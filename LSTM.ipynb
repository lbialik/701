{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "from utils.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Clone the repository\n",
    "# !git clone https://github.com/facebookresearch/colorlessgreenRNNs\n",
    "\n",
    "# # Navigate to the src directory, check its contents\n",
    "# %cd colorlessgreenRNNs/src\n",
    "# !ls\n",
    "\n",
    "# # Get the pretrained model:\n",
    "# !wget https://dl.fbaipublicfiles.com/colorless-green-rnns/best-models/English/hidden650_batch128_dropout0.2_lr20.0.pt\n",
    "\n",
    "# !mkdir ../data/lm/English\n",
    "# !wget -P ../data/lm/English/ https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/train.txt\n",
    "# !wget -P ../data/lm/English/ https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/test.txt\n",
    "# !wget -P ../data/lm/English/ https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/valid.txt\n",
    "# !wget -P ../data/lm/English/ https://dl.fbaipublicfiles.com/colorless-green-rnns/training-data/English/vocab.txt\n",
    "\n",
    "# # %cd colorlessgreenRNNs/src/language_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/liza/Projects/701/colorlessgreenRNNs/src/language_models\n"
     ]
    }
   ],
   "source": [
    "%cd colorlessgreenRNNs/src/language_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liza/miniconda3/envs/701/lib/python3.7/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'model.RNNModel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/liza/miniconda3/envs/701/lib/python3.7/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/liza/miniconda3/envs/701/lib/python3.7/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/liza/miniconda3/envs/701/lib/python3.7/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTM' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/home/liza/miniconda3/envs/701/lib/python3.7/site-packages/torch/serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "torch.manual_seed(50360)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(50360)\n",
    "\n",
    "model_ = None\n",
    "fn = \"../hidden650_batch128_dropout0.2_lr20.0.pt\"\n",
    "with open(fn, \"rb\") as model_f:\n",
    "    model_ = torch.load(fn)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.device('cuda')\n",
    "else:\n",
    "    torch.device('cpu')\n",
    "\n",
    "# Makes sure your model is loaded onto the GPU (should return true)\n",
    "next(model_.parameters()).is_cuda\n",
    "\n",
    "# Finally - load the model.\n",
    "from model import RNNModel\n",
    "\n",
    "# Construct a new RNNModel using PyTorch 1.x implementations of NN modules\n",
    "model = RNNModel(\"LSTM\", 50001, 650, 650, 2, 0.2, False)\n",
    "# Copy over the trained weights from the model loaded in\n",
    "model.load_state_dict(model_.state_dict())\n",
    "\n",
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Import dictionary_corpus, part of the colorlessgreenRNNs repoistory that has some use useful functions\n",
    "# import dictionary_corpus\n",
    "\n",
    "# data_path = \"../../data/lm/English\"\n",
    "# dictionary = dictionary_corpus.Dictionary(data_path)\n",
    "\n",
    "# sentence = \"the key to the cabinets\"\n",
    "# tokenized_sentence = sentence.split()\n",
    "# input = [dictionary.word2idx[w]  if w in dictionary.word2idx\n",
    "#                                     else dictionary.word2idx[\"<unk>\"]\n",
    "#                   for w in tokenized_sentence]\n",
    "\n",
    "# for w in tokenized_sentence:\n",
    "#     if w not in dictionary.word2idx:\n",
    "#         print(w, ' not in vocab!')\n",
    "\n",
    "# input = torch.tensor(input, dtype=torch.long)\n",
    "# input = input.cuda()\n",
    "\n",
    "# ## Extract the hidden and output layers at each input token:\n",
    "# cur_sentence_output, cur_sentence_hidden = model(input.view(-1, 1), # (sequence_length, batch_size).\n",
    "#                        model.init_hidden(1)) # one input at a time, thus batch_size = 1\n",
    "\n",
    "# ## Size of the output for the current sentence: Length of the sequence (in words), batch size (1), and length of the vocabulary (50001)\n",
    "# ## This is the score of each word in the vocabulary (N = 50001) as the next token in the sequence, for each of the 5 input positions.  \n",
    "# cur_sentence_output.size()\n",
    "\n",
    "# ## The output at the final token represents the predictions about the next word in the sequence. \n",
    "# ## We can pull out the next word predictions explicitly, since that's what we care about.\n",
    "# next_word_scores = cur_sentence_output[-1].view(-1)\n",
    "\n",
    "# ## What's the most likely next word?\n",
    "# ## Get the maximum value, and the associated id, with the max method:\n",
    "# predicted_word, predicted_word_id = next_word_scores.max(0)\n",
    "\n",
    "# predicted_word_id = predicted_word_id.item() # get an int out of a 1x1 tensor. Remove this line to see the difference!\n",
    "# print(\"Most likely next word ID:\",predicted_word_id)\n",
    "# print(\"Most likely next word:\", dictionary.idx2word[predicted_word_id])\n",
    "\n",
    "# ## Retrieve the score of a particular word\n",
    "# is_idx = dictionary.word2idx[\"is\"]\n",
    "# are_idx = dictionary.word2idx[\"are\"]\n",
    "\n",
    "# print(\"is score: \", next_word_scores[is_idx].item())\n",
    "# print(\"are score: \", next_word_scores[are_idx].item())\n",
    "\n",
    "# ## Scores are uninterpretable on their own; we might want to convert them \n",
    "# ## to a probability distribution using softmax\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# next_word_dist = F.softmax(next_word_scores, dim=0)\n",
    "\n",
    "# print(\"is prob: \", next_word_dist[is_idx].item())\n",
    "# print(\"are prob: \", next_word_dist[are_idx].item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model and Append to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import dictionary_corpus, part of the colorlessgreenRNNs repoistory that has some use useful functions\n",
    "import dictionary_corpus\n",
    "import torch.nn.functional as F\n",
    "\n",
    "data_path = \"../../data/lm/English\"\n",
    "dictionary = dictionary_corpus.Dictionary(data_path)\n",
    "np.random.seed(50360)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(intro, query):\n",
    "    intro, query = intro.split(), query.split()\n",
    "    for w in intro + query:\n",
    "        if w not in dictionary.word2idx:\n",
    "            print(w, ' not in vocab!')\n",
    "\n",
    "    tokenized_intro = [dictionary.word2idx[w]  if w in dictionary.word2idx\n",
    "                                        else dictionary.word2idx[\"<unk>\"]\n",
    "                    for w in intro]\n",
    "    tokenized_query = [dictionary.word2idx[w]  if w in dictionary.word2idx\n",
    "                                        else dictionary.word2idx[\"<unk>\"]\n",
    "                    for w in query]\n",
    "\n",
    "    for query_token in tokenized_query:\n",
    "\n",
    "        print(f'intro: {[dictionary.idx2word[w] for w in tokenized_intro]}')\n",
    "        print(f'query: {dictionary.idx2word[query_token]}')\n",
    "\n",
    "        input = torch.tensor(tokenized_intro, dtype=torch.long).cuda()\n",
    "\n",
    "        ## Extract the hidden and output layers at each input token:\n",
    "        cur_sentence_output, cur_sentence_hidden = model(input.view(-1, 1), # (sequence_length, batch_size).\n",
    "                            model.init_hidden(1)) # one input at a time, thus batch_size = 1\n",
    "        next_word_scores = cur_sentence_output[-1].view(-1)\n",
    "        \n",
    "        next_word_dist = F.softmax(next_word_scores, dim=0)\n",
    "        print(\"query token prob: \", next_word_dist[query_token].item())\n",
    "        tokenized_intro.append(query_token)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1558e2eed1b9f3ab2153a0b71fa790de882ec11074c52e36ccbd398f0a73bcc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('701')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
